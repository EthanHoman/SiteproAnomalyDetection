@startuml Phase2B_Inference_Sequence


participant "Cron Job" as cron
participant "EdgeInference" as inference
participant "ConfigLoader" as config
participant "ModelLoader" as loader
participant "DebounceManager" as debounce
participant "AnomalyAPIClient" as api
participant "LocalStorage" as storage
participant "Central API" as central
database "File System" as fs

== System Initialization (on startup) ==
cron -> inference: new EdgeInference(artifact_dir=".")

group Load Configurations
  inference -> config: load_deployment_config()
  config -> fs: read config/deployment_config.json
  fs --> config: {api, site_info, anomaly_reporting}
  config --> inference: deployment_config

  inference -> config: load_baseline_parameters()
  config -> fs: read config/baseline.json
  fs --> config: {baseline_flow, baseline_head, ...}
  config --> inference: baseline_parameters

  inference -> config: load_tolerances()
  config -> fs: read config/tolerances.json
  fs --> config: all tolerance categories
  config -> config: select category "1U"
  config --> inference: tolerances for 1U

  inference -> config: load_column_mapping()
  config -> fs: read config/column_mapping.json
  fs --> config: {flow: "Flow (gpm)", ...}
  config --> inference: column_mapping
end

group Load ML Model
  inference -> loader: load_model()
  loader -> fs: read model/anomaly_detector.pkl
  fs --> loader: RandomForestRegressor
  loader --> inference: model

  inference -> loader: load_scaler()
  loader -> fs: read model/scaler.pkl
  fs --> loader: StandardScaler
  loader --> inference: scaler

  inference -> loader: load_metadata()
  loader -> fs: read model/model_metadata.json
  fs --> loader: {version: "1.0.0", model_type: "random_forest"}
  loader --> inference: metadata
end

group Setup API Client
  inference -> api: new AnomalyAPIClient(base_url, bearer_token)
  api -> api: create HTTP session with auth headers
  api --> inference: client ready
end

inference -> debounce: new DebounceManager(debounce_minutes=60)
debounce --> inference: debounce manager ready

== Hourly Inference Run ==
cron -> inference: run_inference("sensor_data.csv", "results.json")

group Process Sensor Data
  inference -> fs: read sensor_data.csv
  fs --> inference: DataFrame with 3 readings
  inference -> inference: parse timestamps
  inference -> inference: sort by timestamp
  inference -> inference: select latest row (current_row)
end

group Calculate Deviations
  loop for each parameter (flow, head, power, efficiency)
    inference -> inference: col = column_mapping[param]
    inference -> inference: baseline = baseline_parameters[f"baseline_{param}"]
    inference -> inference: current = current_row[col]
    inference -> inference: deviation = ((current - baseline) / baseline) * 100
  end
  inference --> inference: deviations = {flow: 15.0, head: 10.0, power: 8.5, efficiency: -1.2}
end

group Check Tolerances
  loop for each parameter
    inference -> inference: max_threshold = tolerances[param]["max_deviation"]
    inference -> inference: min_threshold = tolerances[param]["min_deviation"]

    alt deviation > max_threshold
      inference -> inference: violations[param] = {exceeded: true, type: "max"}
    else deviation < min_threshold AND min_threshold > -999
      inference -> inference: violations[param] = {exceeded: true, type: "min"}
    end
  end

  inference -> inference: classify_status(violations, mandatory_exceeded)

  alt mandatory_exceeded AND severity > 2.0
    inference -> inference: status = "Failure"
  else mandatory_exceeded AND severity > 1.5
    inference -> inference: status = "Critical"
  else mandatory_exceeded
    inference -> inference: status = "Warning"
  else only optional violations
    inference -> inference: status = "Warning"
  else no violations
    inference -> inference: status = "Normal"
  end

  inference --> inference: tolerance_results = {status: "Warning", violations: {...}}
end

group ML Prediction (if enough data)
  alt DataFrame has >= 168 rows
    inference -> inference: extract_features(df)
    inference -> inference: create rolling statistics (24h, 168h)
    inference -> inference: calculate slopes and acceleration
    inference -> inference: features = [64 feature values]
    inference -> inference: features_scaled = scaler.transform(features)
    inference -> inference: rul_prediction = model.predict(features_scaled)
    inference -> inference: rul_days = max(0, rul_prediction[0])
    inference -> inference: calculate confidence and probability
    inference --> inference: prediction = {rul_days: 12.5, probability: 0.85, confidence: 0.87}
  else
    inference --> inference: prediction = None (not enough data)
  end
end

group Determine if Reporting Needed
  inference -> inference: should_report_anomaly(tolerance_results, prediction)

  alt status == "Normal"
    inference --> inference: (False, "Status is Normal")
  else
    inference -> debounce: is_debounced(parameter)
    debounce -> debounce: elapsed = now - last_reported[parameter]

    alt elapsed < 60 minutes
      debounce --> inference: True
      inference --> inference: (False, "Recently reported")
    else
      debounce --> inference: False

      alt mandatory_exceeded
        inference --> inference: (True, "Mandatory parameter exceeded")
      else status in ["Critical", "Failure"]
        inference --> inference: (True, f"Status is {status}")
      else prediction.probability > 0.7 AND prediction.rul_days < 7
        inference --> inference: (True, "High confidence failure prediction")
      else status == "Warning"
        inference --> inference: (True, "Warning status")
      end
    end
  end
end

group Report Anomaly (if should_report = True)
  inference -> inference: format_anomaly_payload(tolerance_results, prediction, current_row)

  inference -> inference: build description
  inference -> inference: description = "Flow exceeded 15.0% (threshold: 10.0%), Head exceeded 10.0% (threshold: 6.0%)"

  inference -> inference: create payload
  note right
    {
      "sourceType": "log",
      "description": "Flow exceeded 15.0%...",
      "siteId": 35482,
      "pumpId": 1,
      "sensorId": 101,
      "timestamp": "2024-07-25T14:32:00Z",
      "logValue": 575.0,
      "additionalContext": {
        "status": "Warning",
        "all_deviations": {...},
        "baseline_values": {...},
        "current_values": {...}
      },
      "metadata": {
        "modelVersion": "1.0.0",
        "confidence": 0.87,
        "prediction_rul_days": 12.5
      }
    }
  end note

  inference -> api: submit_anomaly(payload)

  group Retry Logic with Exponential Backoff
    loop attempt = 1 to 3
      api -> api: validate_payload(payload)

      api -> central: POST /edge/anomalies\nAuthorization: Bearer TOKEN

      alt API Success (200 OK)
        central --> api: {id: 12345, ...payload, createdAt: "..."}
        api --> inference: response

        inference -> debounce: update_tracker(parameter)
        debounce -> debounce: last_reported[parameter] = now
        debounce --> inference: updated

        inference -> inference: log success

        leave
      else API Error (401, 500, timeout)
        central --> api: error response

        alt attempt < 3
          api -> api: delay = retry_delay * (2 ^ attempt)  ' exponential backoff
          api -> api: sleep(delay)
        else attempt == 3
          api -> api: log all retries failed
          api --> inference: raise Exception("All retries failed")

          inference -> storage: save_unsent_anomaly(payload)
          storage -> fs: write unsent_anomalies/anomaly_20240725_143200.json
          fs --> storage: saved
          storage --> inference: saved for retry later
        end
      end
    end
  end
end

group Save Results Locally
  inference -> inference: compile results
  note right
    {
      "timestamp": "2024-07-25T14:32:00",
      "status": "Warning",
      "deviations": {...},
      "violations": {...},
      "prediction": {...},
      "reported_to_api": true
    }
  end note

  inference -> storage: save_result(results)
  storage -> fs: write results.json
  fs --> storage: saved
  storage --> inference: saved
end

inference --> cron: Inference complete\nStatus: Warning\nReported: Yes


@enduml